{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rcali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    remove= re.sub(r'\\d+', '', text)\n",
    "    return remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords & Punctuations\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\", \".join(stopwords.words('english'))\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove mentions and hashtags\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extra white space left while removing stuff\n",
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return space_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r\"<.*?>\",\"\",text)\n",
    "\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return wierd_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        lemma_word = lemmatizer.lemmatize(tokens[i])\n",
    "        tokens[i] = lemma_word\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tweet):\n",
    "    \n",
    "    # remove urls\n",
    "    tweet = re.sub(r'http\\S+', ' ', tweet)\n",
    "    \n",
    "    # remove html tags\n",
    "    tweet = re.sub(r'<.*?>',' ', tweet)\n",
    "    \n",
    "    # remove digits\n",
    "    tweet = re.sub(r'\\d+',' ', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#\\w+',' ', tweet)\n",
    "    \n",
    "    # remove mentions\n",
    "    tweet = re.sub(r'@\\w+',' ', tweet)\n",
    "    \n",
    "    #removing stop words\n",
    "    tweet = tweet.split()\n",
    "    tweet = \" \".join([word for word in tweet if not word in STOPWORDS])\n",
    "    \n",
    "    return  tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_REGEX = re.compile('http(s)?:\\/\\/t.co\\/\\w+')\n",
    "MENTION_REGEX = re.compile('@\\w+')\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # remove mentions, the pound sign, and replace urls with URL token\n",
    "    tweet = re.sub(URL_REGEX, '', tweet)\n",
    "    tweet = re.sub(MENTION_REGEX, '', tweet)\n",
    "    tweet = tweet.replace('#', '')\n",
    "\n",
    "    return tweet.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"‚ú®üéÇ Happy birthday, @KMbappe!‚ù§Ô∏è https://t.co/9ZQYQ5Z7Zr Woke up like this üèÜüò¥ #SGEurogoals #the me I am it is they are \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚ú®üéÇ Happy birthday, !‚ù§Ô∏è  Woke up like this üèÜüò¥ SGEurogoals the me I am it is they are'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = clean_tweet(tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Happy birthday, !  Woke up like this  SGEurogoals the me I am it is they are'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = remove_emojis(tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy birthday, ! Woke like SGEurogoals I'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09a5c1a0785791582b59588a09eb36d766bf3d0e4b89d17beff338cc9d1c7f9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
